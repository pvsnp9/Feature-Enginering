{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Magnitude\n",
    "\n",
    "In Linear Regression models, the scale of variables used to estimate the output matters. Linear models are of the type y = w x + b, where the regression coefficient w represents the expected change in y for a one unit change in x (the predictor). Thus, the magnitude of w is partly determined by the magnitude of the units being used for x. If x is a distance variable, just changing the scale from kilometers to miles will cause a change in the magnitude of the coefficient.\n",
    "\n",
    "In addition, in situations where we estimate the outcome y by contemplating multiple predictors x1, x2, ...xn, predictors with greater numeric ranges dominate over those with smaller numeric ranges.\n",
    "\n",
    "Gradient descent converges faster when all the predictors (x1 to xn) are within a similar scale, therefore having features in a similar scale is useful for Neural Networks as well as.\n",
    "\n",
    "In Support Vector Machines, feature scaling can decrease the time to find the support vectors.\n",
    "\n",
    "Finally, methods using Euclidean distances or distances in general are also affected by the magnitude of the features, as Euclidean distance is sensitive to variations in the magnitude or scales of the predictors. Therefore feature scaling is required for methods that utilise distance calculations like k-nearest neighbours (KNN) and k-means clustering.\n",
    "\n",
    "**Magnitude is important due to following reason**\n",
    "- The regression coefficient is directly influenced by the scale of the variable\n",
    "- Variables with bigger magnitude / value range dominate over the ones with smaller magnitude / value range\n",
    "- Gradient descent converges faster when features are on similar scales\n",
    "- Feature scaling helps decrease the time to find support vectors for SVMs\n",
    "- Euclidean distances are sensitive to feature magnitude.\n",
    "\n",
    "**Feature Magnitude sensitive machine learning models**\n",
    "- Linear and Logistic Regression\n",
    "- Neural Networks\n",
    "- Support Vector Machines\n",
    "- KNN\n",
    "- K-means clustering\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- Principal Component Analysis (PCA)\n",
    "\n",
    "**Feature Magnitude insensitive machine learning models**\n",
    "- Classification and Regression Trees\n",
    "- Random Forests\n",
    "- Gradient Boosted Trees\n",
    "\n",
    "In this notebook:\n",
    "- Feature magnitude \n",
    "\n",
    "Dataset: Titanic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>211.3375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived      age      fare\n",
       "0       1         1  29.0000  211.3375\n",
       "1       1         1   0.9167  151.5500\n",
       "2       1         0   2.0000  151.5500\n",
       "3       1         0  30.0000  151.5500\n",
       "4       1         0  25.0000  151.5500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load titanic dataset with only few columns \n",
    "\n",
    "use_columns = ['age', 'pclass','fare', 'survived']\n",
    "\n",
    "data = pd.read_csv('../datasets/titanic.csv', usecols=use_columns)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1046.000000</td>\n",
       "      <td>1308.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.294882</td>\n",
       "      <td>0.381971</td>\n",
       "      <td>29.881135</td>\n",
       "      <td>33.295479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.837836</td>\n",
       "      <td>0.486055</td>\n",
       "      <td>14.413500</td>\n",
       "      <td>51.758668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>31.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pclass     survived          age         fare\n",
       "count  1309.000000  1309.000000  1046.000000  1308.000000\n",
       "mean      2.294882     0.381971    29.881135    33.295479\n",
       "std       0.837836     0.486055    14.413500    51.758668\n",
       "min       1.000000     0.000000     0.166700     0.000000\n",
       "25%       2.000000     0.000000    21.000000     7.895800\n",
       "50%       3.000000     0.000000    28.000000    14.454200\n",
       "75%       3.000000     1.000000    39.000000    31.275000\n",
       "max       3.000000     1.000000    80.000000   512.329200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Fare varies between 0 and 512, Age between 0 and 80, and Class between 0 and 3. So the variables have different magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (916, 3), Test shape: (393, 3)\n"
     ]
    }
   ],
   "source": [
    "# split the data for training and testing \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[['pclass', 'age', 'fare']].fillna(0),\n",
    "    data.survived,\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Scaling\n",
    "\n",
    "Here, we are going to use MinMax Scalar from Scikit library. It ranges all values in **0-1**\n",
    "\n",
    "**Formula:**\n",
    "Transformation:\n",
    "\n",
    "$X_{rescaled} = X - X.min() / (X.max() - X.min()) $\n",
    "\n",
    "Revert into original one:\n",
    "\n",
    "$X = X_{rescaled} * (max - min) + min $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets scale the feature: MinMax\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [0.64628821 0.33048359 0.06349833]\n",
      "Standard Deviation:  [0.42105785 0.23332045 0.09250036]\n",
      "Minimum value:  [0. 0. 0.]\n",
      "Maximum value:  [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('Mean: ', X_train_scaled.mean(axis=0))\n",
    "print('Standard Deviation: ', X_train_scaled.std(axis=0))\n",
    "print('Minimum value: ', X_train_scaled.min(axis=0))\n",
    "print('Maximum value: ', X_train_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Max value is 1 and Min values is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.6793181006244372\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.7175488081411426\n"
     ]
    }
   ],
   "source": [
    "logitreg = LogisticRegression(\n",
    "        C=1000, #avoid regularization\n",
    "        solver='lbfgs',\n",
    "        random_state=44\n",
    "    )\n",
    "\n",
    "logitreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Train set')\n",
    "pred = logitreg.predict_proba(X_train)\n",
    "print('Logistic Regression roc-auc: {}'.format(\n",
    "    roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Test set')\n",
    "test_pred = logitreg.predict_proba(X_test)\n",
    "print('Logistic Regression roc-auc: {}'.format(\n",
    "    roc_auc_score(y_test, test_pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.71428242, -0.00923013,  0.00425235]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logitreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.6793281640744896\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.7175488081411426\n"
     ]
    }
   ],
   "source": [
    "# Lets used scaled data\n",
    "logitreg = LogisticRegression(\n",
    "        C=1000, #avoid regularization\n",
    "        solver='lbfgs',\n",
    "        random_state=44\n",
    "    )\n",
    "\n",
    "logitreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "print('Train set')\n",
    "pred = logitreg.predict_proba(X_train_scaled)\n",
    "print('Logistic Regression roc-auc: {}'.format(\n",
    "    roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Test set')\n",
    "test_pred = logitreg.predict_proba(X_test_scaled)\n",
    "print('Logistic Regression roc-auc: {}'.format(\n",
    "    roc_auc_score(y_test, test_pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.42875872, -0.68293349,  2.17646757]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logitreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the performance of logistic regression did not change when using the datasets with the features scaled (compare roc-auc values for train and test set for models with and without feature scaling).\n",
    "\n",
    "However, when looking at the coefficients we do see a big difference in the values. This is because the magnitude of the variable was affecting the coefficients. After scaling, all 3 variables have the relatively the same effect (coefficient) towards survival, whereas before scaling, we would be inclined to think that PClass was driving the Survival outcome.\n",
    "\n",
    "##### Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "SVM roc-auc: 0.882393490960506\n",
      "Test set\n",
      "SVM roc-auc: 0.6617581992146452\n"
     ]
    }
   ],
   "source": [
    "SVM_model = SVC(probability=True, gamma='auto',random_state=44)\n",
    "SVM_model.fit(X_train, y_train)\n",
    "\n",
    "print('Train set')\n",
    "pred = SVM_model.predict_proba(X_train)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Test set')\n",
    "pred = SVM_model.predict_proba(X_test)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "coef_ is only available when using a linear kernel",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-3728ca1d7355>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSVM_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\whiskey_pt\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mcoef_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcoef_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m             raise AttributeError('coef_ is only available when using a '\n\u001b[0m\u001b[0;32m    502\u001b[0m                                  'linear kernel')\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: coef_ is only available when using a linear kernel"
     ]
    }
   ],
   "source": [
    "SVM_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "SVM roc-auc: 0.6780802962679695\n",
      "Test set\n",
      "SVM roc-auc: 0.6841435761296388\n"
     ]
    }
   ],
   "source": [
    "SVM_model = SVC(probability=True, gamma='auto',random_state=44)\n",
    "SVM_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Train set')\n",
    "pred = SVM_model.predict_proba(X_train_scaled)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Test set')\n",
    "pred = SVM_model.predict_proba(X_test_scaled)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling improved the performance of the support vector machine. After feature scaling the model is no longer over-fitting to the training set (compare the roc-auc of 0.881 for the model on unscaled features vs the roc-auc of 0.68). In addition, the roc-auc for the testing set increased as well (0.66 vs 0.68).\n",
    "\n",
    "##### K-Nearest Neighbours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "KNN roc-auc: 0.8131141849360215\n",
      "Test set\n",
      "KNN roc-auc: 0.6947901111664178\n"
     ]
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "print('Train set')\n",
    "pred = KNN.predict_proba(X_train)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = KNN.predict_proba(X_test)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "KNN roc-auc: 0.826928785995703\n",
      "Test set\n",
      "KNN roc-auc: 0.7232453957192633\n"
     ]
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "KNN.fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Train set')\n",
    "pred = KNN.predict_proba(X_train_scaled)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = KNN.predict_proba(X_test_scaled)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe for KNN as well that feature scaling improved the performance of the model. The model built on unscaled features shows a better generalisation, with a higher roc-auc for the testing set (0.72 vs 0.69 for model built on unscaled features).\n",
    "\n",
    "Both KNN methods are over-fitting to the train set. Thus, we would need to change the parameters of the model or use less features to try and decrease over-fitting, which exceeds the purpose of this demonstration.\n",
    "\n",
    "##### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9866810238554083\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7326751838946961\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, random_state=39)\n",
    "\n",
    "# train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate performance\n",
    "print('Train set')\n",
    "pred = rf.predict_proba(X_train)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Test set')\n",
    "pred = rf.predict_proba(X_test)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9867917218059866\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7312510370001659\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, random_state=39)\n",
    "\n",
    "# train the model\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# evaluate performance\n",
    "print('Train set')\n",
    "pred = rf.predict_proba(X_train_scaled)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))\n",
    "print('Test set')\n",
    "pred = rf.predict_proba(X_test_scaled)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, Random Forests shows no change in performance regardless of whether it is trained on a dataset with scaled or unscaled features. This model in particular, is over-fitting to the training set. So we need to do some work to remove the over-fitting. That exceeds the scope of this demonstration.\n",
    "\n",
    "##### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "AdaBoost roc-auc: 0.7970629821021541\n",
      "Test set\n",
      "AdaBoost roc-auc: 0.7473867595818815\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=200, random_state=44)\n",
    "\n",
    "# train the model\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# evaluate model performance\n",
    "print('Train set')\n",
    "pred = ada.predict_proba(X_train)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = ada.predict_proba(X_test)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "AdaBoost roc-auc: 0.7970629821021541\n",
      "Test set\n",
      "AdaBoost roc-auc: 0.7475250262706707\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=200, random_state=44)\n",
    "\n",
    "# train the model\n",
    "ada.fit(X_train_scaled, y_train)\n",
    "\n",
    "# evaluate model performance\n",
    "print('Train set')\n",
    "pred = ada.predict_proba(X_train_scaled)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = ada.predict_proba(X_test_scaled)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, AdaBoost shows no change in performance regardless of whether it is trained on a dataset with scaled or unscaled features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f1fe20136675b47db30784a64b23f2f74600804c7b298c8fa67f64368253755"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('whiskey_pt': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
